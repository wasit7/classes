{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AsyncCrawler\n",
    "\n",
    "from http://edmundmartin.com/writing-a-web-crawler-in-python-3-5-using-asyncio/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import datetime\n",
    "import aiohttp\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from lxml import html as lh\n",
    "from tqdm import tqdm, tnrange\n",
    " \n",
    "class AsyncCrawler:\n",
    "\n",
    "    def __init__(self, start_url, count, max_concurrency=200):\n",
    "        self.start_url = start_url\n",
    "        self.base_url = '{}://{}'.format(urlparse(self.start_url).scheme, urlparse(self.start_url).netloc)\n",
    "        self.count = count\n",
    "        self.seen_urls = set()\n",
    "        self.bounded_sempahore = asyncio.BoundedSemaphore(max_concurrency)\n",
    "        print(f'{datetime.now()} [start crawling]',end='\\r')\n",
    "\n",
    "    async def http_request(self, url):\n",
    "        print(f'{datetime.now()} {url:200}',end='\\r')\n",
    "        async with self.bounded_sempahore:\n",
    "            try:\n",
    "                async with self.session.get(url, timeout=10) as response:\n",
    "                    html = await response.read()\n",
    "                    return html\n",
    "            except Exception as e:\n",
    "                pass\n",
    " \n",
    "    def find_urls(self, html):\n",
    "        found_urls = []\n",
    "        dom = lh.fromstring(html)\n",
    "        for href in dom.xpath('//a/@href'):\n",
    "            url = urljoin(self.base_url, href)\n",
    "            if url not in self.seen_urls and url.startswith(self.base_url):\n",
    "                found_urls.append(url)\n",
    "        return found_urls\n",
    "    \n",
    "    async def extract_async(self, url):\n",
    "        data = await self.http_request(url)\n",
    "        found_urls = set()\n",
    "        if data:\n",
    "            for url in self.find_urls(data):\n",
    "                found_urls.add(url)\n",
    "                return url, data, sorted(found_urls)\n",
    "            \n",
    "    async def extract_multi_async(self, to_fetch):\n",
    "        futures, results = [], []\n",
    "        for url in to_fetch:\n",
    "            if url in self.seen_urls: continue\n",
    "            self.seen_urls.add(url)\n",
    "            futures.append(self.extract_async(url))\n",
    "\n",
    "        for future in asyncio.as_completed(futures):\n",
    "            try:\n",
    "                results.append((await future))\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        return results\n",
    "    \n",
    "    def parser(self, data):\n",
    "        dom = lh.fromstring(data)\n",
    "        title = dom.cssselect('title')\n",
    "        if title:\n",
    "            title = title[0].text\n",
    "        return {'title': title}\n",
    "\n",
    "    async def crawl_async(self):\n",
    "        to_fetch = [self.start_url]\n",
    "        results=[]\n",
    "        self.session = aiohttp.ClientSession()\n",
    "        for c in tnrange(self.count):\n",
    "            batch = await self.extract_multi_async(to_fetch)\n",
    "            to_fetch = []\n",
    "            for url, data, found_urls in batch:\n",
    "                data = self.parser(data)\n",
    "                results.append((c, url, data))\n",
    "                to_fetch.extend(found_urls)\n",
    "        await self.session.close()\n",
    "        return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-18 01:13:30.371760 [start crawling]\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab03f128ac14b05ae55bdb8d7f014e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-18 01:13:49.878193 https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes                                                                                                                                                   \n"
     ]
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Wiki'\n",
    "crawler = AsyncCrawler(url, 100)\n",
    "future = asyncio.Task(crawler.crawl_async())\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "try:\n",
    "    loop.run_until_complete(future)\n",
    "    loop.close()\n",
    "    result = future.result()\n",
    "    print(len(result))\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
